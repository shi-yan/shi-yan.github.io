<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!-- Primary Meta Tags -->
    <title>Shi&#x27;s blog: How Does FLAME Work</title>
    <meta name="title" content="How Does FLAME Work" />
    <meta name="description" content="This post explores the implementation of the FLAME model, used for 3D head modeling and animation. It covers key steps such as joint transformations, pose adjustments, and handling dynamic facial landmarks. Along the way, it clarifies ambiguities in the FLAME and RingNet papers and addresses practical challenges in the process." />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://shi-yan.github.io/how_does_flame_work" />
    <meta property="og:title" content="How Does FLAME Work" />
    <meta property="og:description" content="This post explores the implementation of the FLAME model, used for 3D head modeling and animation. It covers key steps such as joint transformations, pose adjustments, and handling dynamic facial landmarks. Along the way, it clarifies ambiguities in the FLAME and RingNet papers and addresses practical challenges in the process." />
    <meta property="og:image" content="https://shi-yan.github.io/how_does_flame_work/thumb_FLAME_masks.gif" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:url" content="https://shi-yan.github.io/how_does_flame_work" />
    <meta property="twitter:title" content="How Does FLAME Work" />
    <meta property="twitter:description" content="This post explores the implementation of the FLAME model, used for 3D head modeling and animation. It covers key steps such as joint transformations, pose adjustments, and handling dynamic facial landmarks. Along the way, it clarifies ambiguities in the FLAME and RingNet papers and addresses practical challenges in the process." />
    <meta property="twitter:image" content="https://shi-yan.github.io/how_does_flame_work/thumb_FLAME_masks.gif" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>


    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/base16/default-dark.min.css"
        integrity="sha512-EF2rc4QyBiRAGUMVm+EjFPBbdVGaN/pwZhtuKyrC/dM+hcwTxI5BsEDUkrMRI77z4VlDAt/qVopePXB5+ZZ8Gw=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />

    <link rel="alternate" type="application/rss+xml" title="Shi&#x27;s blog" href="https://shi-yan.github.io/rss.xml" />

    <link rel="stylesheet" href="/style.css" />

    <script src="
        https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js
        "></script>
    <link href="
        https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css
        " rel="stylesheet" />
</head>

<body>
    <div id="page">
        <div id="header"><a class="icon" href="/">Shi&#x27;s blog</a> <a class="icon" href="https://github.com/shi-yan"><svg
                    xmlns="http://www.w3.org/2000/svg" height="32" width="32" fill="#dadadb"
                    viewBox="0 0 480 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z" />
                </svg></a>
            <a class="icon" href="https://shi-yan.github.io/rss.xml" target="_blank"><svg xmlns="http://www.w3.org/2000/svg"
                    height="32" width="32" fill="#dadadb"
                    viewBox="0 0 448 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M0 64C0 46.3 14.3 32 32 32c229.8 0 416 186.2 416 416c0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96C14.3 96 0 81.7 0 64zM0 416a64 64 0 1 1 128 0A64 64 0 1 1 0 416zM32 160c159.1 0 288 128.9 288 288c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224c-17.7 0-32-14.3-32-32s14.3-32 32-32z" />
                </svg>
            </a>
        </div>
        <div id="left"></div>
        <div id="content">
            <div id="title">How Does FLAME Work</div>
            <div id="meta"><svg xmlns="http://www.w3.org/2000/svg" height="12" width="11" fill="#dadadb"
                    viewBox="0 0 448 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M152 24c0-13.3-10.7-24-24-24s-24 10.7-24 24V64H64C28.7 64 0 92.7 0 128v16 48V448c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V192 144 128c0-35.3-28.7-64-64-64H344V24c0-13.3-10.7-24-24-24s-24 10.7-24 24V64H152V24zM48 192h80v56H48V192zm0 104h80v64H48V296zm128 0h96v64H176V296zm144 0h80v64H320V296zm80-48H320V192h80v56zm0 160v40c0 8.8-7.2 16-16 16H320V408h80zm-128 0v56H176V408h96zm-144 0v56H64c-8.8 0-16-7.2-16-16V408h80zM272 248H176V192h96v56z" />
                </svg> 2024-12-10 <svg xmlns="http://www.w3.org/2000/svg" style="margin-left:16px;" height="12" width="11"
                    fill="#dadadb"
                    viewBox="0 0 512 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M345 39.1L472.8 168.4c52.4 53 52.4 138.2 0 191.2L360.8 472.9c-9.3 9.4-24.5 9.5-33.9 .2s-9.5-24.5-.2-33.9L438.6 325.9c33.9-34.3 33.9-89.4 0-123.7L310.9 72.9c-9.3-9.4-9.2-24.6 .2-33.9s24.6-9.2 33.9 .2zM0 229.5V80C0 53.5 21.5 32 48 32H197.5c17 0 33.3 6.7 45.3 18.7l168 168c25 25 25 65.5 0 90.5L277.3 442.7c-25 25-65.5 25-90.5 0l-168-168C6.7 262.7 0 246.5 0 229.5zM144 144a32 32 0 1 0 -64 0 32 32 0 1 0 64 0z" />
                </svg>
                <a class="tag" href="/tags/artificial-intelligence">#artificial intelligence</a>,
                <a class="tag" href="/tags/computer-graphics">#computer graphics</a>,
                <a class="tag" href="/tags/machine-learning">#machine learning</a>,
                <svg xmlns="http://www.w3.org/2000/svg" style="margin-left:16px;" height="12" width="11" fill="#dadadb"
                    viewBox="0 0 384 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M320 464c8.8 0 16-7.2 16-16V160H256c-17.7 0-32-14.3-32-32V48H64c-8.8 0-16 7.2-16 16V448c0 8.8 7.2 16 16 16H320zM0 64C0 28.7 28.7 0 64 0H229.5c17 0 33.3 6.7 45.3 18.7l90.5 90.5c12 12 18.7 28.3 18.7 45.3V448c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V64z" />
                </svg>
                1952 Words
                <svg xmlns="http://www.w3.org/2000/svg" style="margin-left:16px;" height="12" width="11" fill="#dadadb"
                    viewBox="0 0 512 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M464 256A208 208 0 1 1 48 256a208 208 0 1 1 416 0zM0 256a256 256 0 1 0 512 0A256 256 0 1 0 0 256zM232 120V256c0 8 4 15.5 10.7 20l96 64c11 7.4 25.9 4.4 33.3-6.7s4.4-25.9-6.7-33.3L280 243.2V120c0-13.3-10.7-24-24-24s-24 10.7-24 24z" />
                </svg>
                8 Mins
            </div>
            <article id="post"><p>I recently had an engaging experience with a <a class="link" href="https://www.tavus.io/" target="_blank">real-time AI talking head demo</a> and became curious about developing something similar myself. Many relevant papers mention using the <a class="link" href="https://flame.is.tue.mpg.de/" target="_blank">FLAME</a> model as a proxy model to seed Gaussian splats. Although I've heard of FLAME for years, I hadn't explored it in depth until now.</p><p>FLAME is a flexible parametric head model capable of capturing variant poses, races, and expressions. With approximately 400 parameters, it can generate a comprehensive head model. Most of these parameters are learned from 3D scanned data using the machine learning technique Principal Component Analysis (PCA), which makes them challenging to interpret directly. A few additional parameters are related to pose variations.</p><p>The process of generating a 3D model in FLAME involves a sophisticated blend of shape blending and skeleton-based morphing. While the FLAME paper lacks comprehensive details, its predecessor, the <a class="link" href="https://smpl.is.tue.mpg.de/" target="_blank">SMPL</a> paper (which focuses on a full-body parametric model), provides valuable insights into the underlying principles.</p><p>Unfortunately, <a class="link" href="https://github.com/soubhiksanyal/FLAME_PyTorch" target="_blank">FLAME's codebase</a> is not well-maintained. The existing implementations often don't work out of the box due to deprecated Python versions or outdated dependencies. Although some community members have attempted to patch the code, crucial fixes remain unmerged due to the project's inactivity. Researchers interested in using FLAME will likely need to investigate pending pull requests to resolve code-related issues.</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="thumb_overview.png" original_src="overview.png" alt="FLAME Overview" sources='["https://smpl.is.tue.mpg.de"]' /><div class="img-title">FLAME Overview<a class="img-source" target="_blank" href="https://smpl.is.tue.mpg.de">[SOURCE]</a></div></div></p><p>The big picture of how FLAME works is summarized well by the above picture. There are four steps overall. First, we create a shape in the neutral pose reflecting the identity and expression. Second, we need to adjust the joint positions according to the identity, because different people have different joint positions. Next, we will further adjust the base shape by applying an additional layer of displacement. This displacement accounts for shape changes caused by posing, for example the creases and wrinkles related to bending a joint. These features can't be simply modeled by skeleton-based local rigid transformations. Finally, we do the skeleton-based morphing to create large movements, like jaw opening or neck turning. There are four joints in total plus a global transformation of the root joint. The four joints are jaw, neck, right and left eyes. Since the FLAME model is untextured, the effects of rotating the right and left eyes are not very visible, hence what really matter are the root, jaw, and neck joints.</p><p>The first step is obtaining a base shape. This base shape should reflect the head's identity, such as age, race, and gender, as well as its expression. This step is purely based on blending shapes. A blending shape can be viewed as a weighted average of a set of shapes sharing the same topology, meaning only the vertex positions are different among these shapes, while the number of triangles and their connectivity remain the same.</p><p>In its implementation, there is a template model, called vTemplate, which can be viewed as the mean of all faces in a neutral pose and expression. This template model has approximately 5,000 vertices and 9,000 triangles.</p><p>There are 400 parameters to alter the model's identity and expression. The first 300 control the identity, and the last 100 change the expression. Since they are learned using Principal Component Analysis (PCA), it can be observed that parameters at the beginning always have a larger effect on the model than those at the end. Consequently, some FLAME programs only allow adjusting the first several parameters for simplicity.</p><p>With a set of 400 parameters, the goal is to calculate vertex displacement or offsets. This is obtained through matrix multiplication. The matrix, called the shape displacement, is <code class="language-math math-inline">V \times 3 \times 400</code> in shape (V is the number of vertices). Multiplying it with the 400 parameters as a vector yields vertex offsets of the shape <code class="language-math math-inline">V \times 3</code>. These offsets are then added to the template model.</p><p>The resulting model has expression, but its pose remains neutral—its neck doesn't turn, and its jaw doesn't open. The subsequent steps will handle the pose component.</p><p>As the second step, we need to calculate the position of the joints. There are four joints: the neck, the jaw, the right and left eyes. In addition, we also define the global transformation in the root joint. The position of the joints is correlated to the shape of the face, hence we perform this step after obtaining the blended shape. The joint positions are also calculated using matrix multiplication <code class="language-math math-inline">J_regressor \times S</code>, where J_regressor is a <code class="language-math math-inline">J \times V</code> matrix, J is the number of joints (5), and the shape <code class="language-math math-inline">S</code> is a <code class="language-math math-inline">V \times 3</code> matrix.</p><p>After determining the joint positions, the next step is to generate the pose feature. Joint poses are provided as a set of vectors, where the direction of a vector specifies the rotation axis and its norm indicates the rotation angle. The pose feature is constructed by subtracting the identity matrix from each of the rotation matrices corresponding to the joints, excluding the root pose. Excluding the root pose makes sense because the pose feature is intended to capture vertex displacements caused by local pose changes. The root pose represents the global transformation of the head and should not influence vertex-level displacements. However, the reasoning behind subtracting the identity matrix from the rotation matrices is less intuitive.</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="posefeature.png" alt="In the Paper, They Remove the Zero Pose From the Feature Vector" sources='["https://smpl.is.tue.mpg.de"]' /><div class="img-title">In the Paper, They Remove the Zero Pose From the Feature Vector<a class="img-source" target="_blank" href="https://smpl.is.tue.mpg.de">[SOURCE]</a></div></div></p><p>In the <a class="link" href="https://smpl.is.tue.mpg.de/" target="_blank">SMPL</a> paper, the pose feature includes a <code class="language-math math-inline">-\textit{R}(\theta^*)</code> term, where <code class="language-math math-inline">\theta^*</code> represents the zero pose, i.e., no rotation, corresponding to the identity matrix. This suggests that the pose feature is designed to only related to deviations from the identity matrix, isolating the effect of relative rotations rather than absolute pose configurations?</p><p>The rotation matrices are computed using the Rodrigues formula:</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="rodrigues.png" alt="The Rodrigues Formula" sources='["https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula"]' /><div class="img-title">The Rodrigues Formula<a class="img-source" target="_blank" href="https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula">[SOURCE]</a></div></div></p><p>In this formula, <code class="language-math math-inline">K</code> is a matrix derived from the rotation axis, and <code class="language-math math-inline">\theta</code> is the rotation angle, equivalent to the norm of the pose vector. The <code class="language-math math-inline">K</code> matrix is defined by the components of the rotation axis vector.</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="K.png" alt="K Is Defined by the Rotation Axis" sources='["https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula"]' /><div class="img-title">K Is Defined by the Rotation Axis<a class="img-source" target="_blank" href="https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula">[SOURCE]</a></div></div></p><p>After converting the four pose vectors (excluding the root pose) into their respective rotation matrices, the resulting matrices are flattened into a single vector containing <code class="language-math math-inline">4 \times 9</code> elements. This vector forms the pose feature.</p><p>The pose feature is then used to compute an additional vertex displacement, referred to as the pose displacement. Human heads are covered in soft tissues that deform under various poses, such as wrinkles forming around the mouth or eyes. These deformations cannot be effectively modeled using skeleton-based transformations alone. The pose displacement accounts for these subtler, pose-dependent changes.</p><p>The pose displacement is calculated by multiplying the pose feature, a <code class="language-math math-inline">P \times 1</code> vector, with the pose displacement matrix, which has dimensions <code class="language-math math-inline">P \times V \times 3</code>. This operation produces vertex-level displacements that are added to the shape to model the effects of pose-specific deformations.</p><p>The final step involves skeleton morphing, which is more challenging to explain conceptually. While the code for this process is straightforward, I have not spent much time reasoning through the details. As such, I will focus on describing the procedure rather than its underlying rationale.</p><p>The first task in this step is to compute the relative position of each joint with respect to its parent joint. The joint positions we obtained earlier are expressed in absolute coordinates. To convert these to relative coordinates, the position of the parent joint is subtracted from the position of the current joint for all joints except the root.</p><p>Next, a transformation matrix is constructed for each joint, incorporating both its rotation and position. This transformation matrix is derived by concatenating the joint's rotation matrix with its relative position vector. This step corresponds to the transformation matrix equation described in the <a class="link" href="https://smpl.is.tue.mpg.de/" target="_blank">SMPL</a> paper:</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="transformation_matrix.png" alt="A Transformation Matrix Is Created by Concatenating the Rotation Matrix and the Relative Position Vector of a Joint" sources='["https://smpl.is.tue.mpg.de"]' /><div class="img-title">A Transformation Matrix Is Created by Concatenating the Rotation Matrix and the Relative Position Vector of a Joint<a class="img-source" target="_blank" href="https://smpl.is.tue.mpg.de">[SOURCE]</a></div></div></p><p>To compute global transformations from these local transformations, the effects of each joint’s transformation matrix are accumulated through matrix multiplication with all its parent matrices. This accumulation ensures that the transformation at any joint reflects the combined influence of its local transformation and all preceding transformations in the hierarchy.</p><p>One aspect that I find unclear, and which the paper does not address adequately, is the question of the rotation origin. As we know, a pose transformation is defined by a rotation axis and an angle, but determining how to apply the rotation also requires specifying the rotation origin. The most intuitive choice for the origin would be the joint itself. Using this assumption, the transformation would involve first translating the joint to the origin, applying the rotation, and then translating it back to its original position. This could be represented mathematically as:</p><p class="katex-display-counter"><code class="language-math math-block">T \times R \times T^\prime</code></p><p>where <code class="language-math math-inline">T^\prime</code> is the translation matrix that moves the joint to the origin, and <code class="language-math math-inline">T</code> moves the joint back to its original position. However, I do not understand the reasoning behind the approach described in the paper, where a transformation matrix is simply constructed by concatenating a joint’s relative position and its rotation. This method does not seem to account for the rotation origin explicitly, which leaves its intuition unclear to me.</p><p>After this step, we obtain a set of global transformation matrices. The final column of each matrix provides the positions of the pose-adjusted joints. These adjusted joint positions, however, serve no functional purpose beyond visualization.</p><p>And finally, the process concludes with applying skeleton-based transformations. This step is the most difficult to understand. Using the global transformations computed earlier, the code subtracts them with <code class="language-math math-inline">T \times J</code>, where <code class="language-math math-inline">T</code> is a joint’s global transformation matrix and <code class="language-math math-inline">J</code> is the homogeneous vector of the joint's position. There are a few points to clarify. First, the joint positions used here are absolute, not relative. Second, while <code class="language-math math-inline">J</code> is described as the homogeneous coordinates of the joint position, its last element is actually zero instead of one. This step is implemented in the code as follows:</p><pre><code class="language-python code-block">rel_transforms = transforms - F.pad(
        torch.matmul(transforms, joints_homogen), [3, 0, 0, 0, 0, 0, 0, 0])</code></pre><p>This implementation corresponds to the following equation in the SMPL paper:</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="thumb_rigid_transformation.png" original_src="rigid_transformation.png" alt="The Paper's Explanation of Removing the Transformation Due to Rest Pose" sources='["https://smpl.is.tue.mpg.de"]' /><div class="img-title">The Paper's Explanation of Removing the Transformation Due to Rest Pose<a class="img-source" target="_blank" href="https://smpl.is.tue.mpg.de">[SOURCE]</a></div></div></p><p>The subtraction term is described as "removing the transformation due to the rest pose." However, I do not fully understand what this means. It might refer to a concept commonly known in computer animation, an area in which I lack sufficient background.</p><p>In the paper, this process of removing the transformation due to the rest pose is represented as applying an inverse matrix, <code class="language-math math-inline">G^{-1}</code>. However, in the code, it is implemented through the subtraction shown above.</p><p>Once the above transformations are obtained, using them to calculate the final shape is relatively straightforward.</p><p>The FLAME model is often applied to trace head position, shape, and expression in 2D images or videos. Given a 2D human head image, the goal is to derive parameters that match the FLAME model to the image. Direct comparison in pixel space can be too challenging, so the <a class="link" href="https://ringnet.is.tue.mpg.de" target="_blank">RingNet paper</a> instead utilizes facial landmarks. To achieve this, we also need to extract facial landmarks from the FLAME model. Unfortunately, RingNet's implementation is part of another <a class="link" href="https://github.com/soubhiksanyal/RingNet" target="_blank">poorly maintained python codebase</a>, which I eventually gave up trying to run.</p><p>In the RingNet paper, facial landmarks are categorized into static and dynamic landmarks, a distinction that was initially confusing. Static landmarks are straightforward; features like the nose and most other facial landmarks are static. The dynamic landmark, however, refers specifically to the cheek line. Its "dynamic" nature arises because it depends on the viewing angle.</p><p>As shown in the image below, when viewing a face from the side, only a segment of the cheek line corresponds to the actual contour of the face. The cheek line on the far side of the face becomes occluded, and the dynamic landmark adapts to represent the visible silhouette instead.</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="thumb_IbkJ8.png" original_src="IbkJ8.png" alt="When One Side of the Cheek Line Is Occluded, the Dynamic Landmark Becomes the Silhouette" sources='["https://cs.stackexchange.com/questions/109227/how-to-detect-facial-landmarks-using-haar-and-other-way"]' /><div class="img-title">When One Side of the Cheek Line Is Occluded, the Dynamic Landmark Becomes the Silhouette<a class="img-source" target="_blank" href="https://cs.stackexchange.com/questions/109227/how-to-detect-facial-landmarks-using-haar-and-other-way">[SOURCE]</a></div></div></p><p>In the code, both static and dynamic landmarks are provided as IDs of the triangles containing a landmark point, along with the barycentric coordinates of those points.</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="thumb_dynamic_landmarks.png" original_src="dynamic_landmarks.png" alt="Each Landmark Contains an Array of Positions" sources='["https://youtu.be/6wPQaJBgreE"]' /><div class="img-title">Each Landmark Contains an Array of Positions<a class="img-source" target="_blank" href="https://youtu.be/6wPQaJBgreE">[SOURCE]</a></div></div></p><p>For dynamic landmarks, each landmark point consists of an array of potential positions. To visualize these landmarks, one position must be selected from the array for each point. The selection process depends on the current viewing angle.</p><p><div class="img-container"><img class="img" onclick="openImage(this)" src="thumb_FLAME_masks.gif" original_src="FLAME_masks.gif" alt="Mask Areas of the FLAME Model" sources='["https://flame.is.tue.mpg.de"]' /><div class="img-title">Mask Areas of the FLAME Model<a class="img-source" target="_blank" href="https://flame.is.tue.mpg.de">[SOURCE]</a></div></div></p><p>Finally, FLAME includes a mask file that specifies various facial regions, represented as sets of face IDs corresponding to those regions.</p></article>
            <div class="older_newer_link_section">
                <div class="older_newer_link_left">
                    <p>


                        <svg xmlns="http://www.w3.org/2000/svg" height="12" width="11" fill="#dadadb"
                            viewBox="0 0 448 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                            <path
                                d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z" />
                        </svg>
                        PREV POST


                    </p>
                    <p>
                        <a class="older_newer_link" href="/launch_webgpu_unleashed">Launch WebGPU Unleashed</a>
                    </p>
                </div>

            </div>
            <a href="https://github.com/shi-yan/shi-yan.github.io/discussions" target="_blank" class="comment"><svg
                    style="margin-right:10px;vertical-align: middle;" xmlns="http://www.w3.org/2000/svg" height="32"
                    width="32" fill="#dadadb"
                    viewBox="0 0 480 512"><!--!Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.-->
                    <path
                        d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z" />
                </svg> Leave a Comment on Github</a>
        </div>
        <div id="right"></div>
        <div id="footer">Generated by <a class="link" href="https://github.com/shi-yan/QuipQuick"
                target="_blank">QuipQuick</a> @</div>
        <!-- The Modal -->
        <div id="img-modal" class="modal">
            <!-- The Close Button -->
            <span class="close" id="img-close">&times;</span>
            <!-- Modal Content (The Image) -->
            <img class="modal-content" id="img01">

            <!-- Modal Caption (Image Text) -->
            <div id="caption"></div>
        </div>
        <script>
            function openImage(img) {
                let modal = document.getElementById("img-modal");
                let modalImg = document.getElementById("img01");
                modal.style.display = "block";
                if (img.getAttribute("original_src")) {
                    modalImg.src = img.getAttribute("original_src");
                } else {
                    modalImg.src = img.src;
                }
                let captionText = document.getElementById("caption");

                captionText.innerText = img.alt;

                let sources = JSON.parse(img.getAttribute("sources"));

                for (let i = 0; i < sources.length; ++i) {
                    if (sources.length == 1) {
                        captionText.innerHTML += "<a href='" + sources[i] + "' target='_blank' class='img-source'>SOURCE</a>";
                    } else {
                        captionText.innerHTML += "<a href='" + sources[i] + "' target='_blank' class='img-source'>SOURCE " + (i + 1) + "</a>";
                    }
                }
            }
            // Get the <span> element that closes the modal
            var closeButton = document.getElementById("img-close");

            // When the user clicks on <span> (x), close the modal
            closeButton.onclick = function () {
                var modal = document.getElementById("img-modal");
                modal.style.display = "none";
            }
        </script>
        <script type="module">
            const macros = {};
            const mathElementsBlock = document.getElementsByClassName("math-block");
            for (let element of mathElementsBlock) {
                katex.render(element.textContent, element, {
                    throwOnError: false,
                    displayMode: true,
                    macros
                });
            }

            const mathElementsInline = document.getElementsByClassName("math-inline");
            for (let element of mathElementsInline) {
                katex.render(element.textContent, element, {
                    throwOnError: false,
                    macros
                });
            }

            hljs.highlightAll();

                    async function setBuildTime() {
                        let response = await fetch("/current_time.txt");
                        let currentTime = await response.text();
                        document.getElementById('footer').innerHTML += ' ' + currentTime;
                    }
                    setBuildTime();
        </script>
        <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0N1753LT3Y"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag() { dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'G-0N1753LT3Y');
</script>
    </div>
</body>

</html>